<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, minimum-scale=1.0">
<title>Multivariate Statistical Analysis</title>

	<meta name="description" content="">


<link rel="icon" href="/favicon.ico" type="image/x-icon">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,400i,500,500i,700,700i|Noto+Sans:400,400i,700,700i|Source+Code+Pro&amp;subset=latin-ext">
<link rel="stylesheet" href="/assets/css/custom_styles.css">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
<link rel="stylesheet" href="/doks-theme/assets/css/style.css">

	</head>
	<body class="green" data-spy="scroll" data-target=".js-scrollspy">
		
	<div class="offcanvas visible-xs">
		<ul class="offcanvas__nav">
			
				<li><a href="/">Home</a></li>
			
				<li><a href="/schedule">Schedule</a></li>
			
		</ul><!-- /.offcanvas__nav -->
	</div><!-- /.offcanvas -->



	<header class="site-header">
      <!--  <a href="https://github.com/qosf/qosf.org"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_white_ffffff.png" alt="Fork me on GitHub"></a>
 -->
		<div class="container">
			<div class="row">
				<div class="col-xs-12">
					
						<a href="/" class="site-header__logo">
                            <img src="/assets/img/logos/qosf_colour_logo.svg">
						</a><!-- /.site-header__logo -->
					
					
						<ul class="site-header__nav hidden-xs">
							
								<li><a href="/">Home</a></li>
							
								<li><a href="/schedule">Schedule</a></li>
							
						</ul><!-- /.site-header__nav -->
						<button class="offcanvas-toggle visible-xs">
							<span></span>
							<span></span>
							<span></span>
						</button><!-- /.offcanvas-toggle -->
					
				</div><!-- /.col -->
			</div><!-- /.row -->
		</div><!-- /.container -->
	</header><!-- /.site-header -->


		<div class="hero-subheader">
            
            
            
			<div class="container">
				<div class="row">
					<div class="col-md-7">
						<div class="align-container" data-mh>
							<div class="align-inner">
								
									<h1 class="hero-subheader__title">Multivariate Statistical Analysis</h1>
								
								
                                <p class="hero-subheader__desc">
</p>
								
                                
							</div><!-- /.align-inner -->
						</div><!-- /.align-container -->
					</div><!-- /.col -->
					
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.hero-subheader -->
		<div class="section">
			<div class="container">
				<div class="row">
					<div class="col-md-7">
						<div class="content">
							<p>Find <a href="https://teachresources.github.io/RPP/u9/pca.Rmd">here</a> the
correspondent <code class="language-plaintext highlighter-rouge">*Rmd</code>. This tutorial is from
<a href="http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/">here</a>.</p>

<h1 id="principal-component-methods-in-r">Principal Component Methods in R</h1>

<p>Principal component methods are used to summarize and visualize the
information contained in a large multivariate data sets. Here, we
provide practical examples and course videos to compute and interpret
principal component methods (PCA, CA, MCA, MFA, etc) using R software.</p>

<p>The following figure illustrates the type of analysis to be performed
depending on the type of variables contained in the data set.</p>

<p><img src="/u9/multivariate-analysis-factoextra.png" alt="" /></p>

<h1 id="principal-component-analysis">Principal component analysis</h1>

<p>Principal component analysis (PCA) allows us to summarize and to
visualize the information in a data set containing
individuals/observations described by multiple inter-correlated
quantitative variables. Each variable could be considered as a different
dimension. If you have more than 3 variables in your data sets, it could
be very difficult to visualize a multi-dimensional hyperspace.</p>

<p>Principal component analysis is used to extract the important
information from a multivariate data table and to express this
information as a set of few new variables called principal components.
These new variables correspond to a linear combination of the originals.
The number of principal components is less than or equal to the number
of original variables.</p>

<p>The information in a given data set corresponds to the total variation
it contains. The goal of PCA is to identify directions (or principal
components) along which the variation in the data is maximal.</p>

<p>In other words, PCA reduces the dimensionality of a multivariate data to
two or three principal components, that can be visualized graphically,
with minimal loss of information.</p>

<h2 id="basics">Basics</h2>

<p>Understanding the details of PCA requires knowledge of linear algebra.
Here, we’ll explain only the basics with simple graphical representation
of the data.</p>

<p>In the Plot 1A below, the data are represented in the X-Y coordinate
system. The dimension reduction is achieved by identifying the principal
directions, called principal components, in which the data varies.</p>

<p>PCA assumes that the directions with the largest variances are the most
“important” (i.e, the most principal).</p>

<p>In the figure below, the <code class="language-plaintext highlighter-rouge">PC1 axis</code> is the <code class="language-plaintext highlighter-rouge">first principal direction</code>
along which the samples show the largest variation. The <code class="language-plaintext highlighter-rouge">PC2 axis</code> is
the <code class="language-plaintext highlighter-rouge">second most important direction</code> and it is <code class="language-plaintext highlighter-rouge">orthogonal</code> to the PC1
axis.</p>

<p>The dimensionality of our two-dimensional data can be reduced to a
single dimension by projecting each sample onto the first principal
component (Plot 1B)</p>

<p><img src="/u9/pca1.png" alt="" /> <img src="/u9/pca2.png" alt="" /> Technically speaking, the amount of
variance retained by each principal component is measured by the
so-called <strong>eigenvalue</strong>.</p>

<p>Note that, the PCA method is particularly useful when the variables
within the data set are highly correlated. Correlation indicates that
there is redundancy in the data. Due to this redundancy, PCA can be used
to reduce the original variables into a smaller number of new variables
( = <strong>principal components</strong>) explaining most of the variance in the
original variables.</p>

<p><img src="/u9/pca3.png" alt="" /> <img src="/u9/pca4.png" alt="" /></p>

<p>Taken together, the main purpose of principal component analysis is to:</p>

<ul>
  <li>identify hidden pattern in a data set,</li>
  <li>reduce the dimensionnality of the data by removing the noise and
redundancy in the data,</li>
  <li>identify correlated variables</li>
</ul>

<h2 id="r-packages">R Packages</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>library("FactoMineR")
library("factoextra")

## Loading required package: ggplot2

## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa
</code></pre></div></div>

<h2 id="data-for-the-tutorial">Data for the tutorial</h2>

<p>We’ll use the demo data sets decathlon2 from the factoextra package:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data(decathlon2)
head(decathlon2)

##           X100m Long.jump Shot.put
## SEBRLE    11.04      7.58    14.83
## CLAY      10.76      7.40    14.26
## BERNARD   11.02      7.23    14.25
## YURKOV    11.34      7.09    15.19
## ZSIVOCZKY 11.13      7.30    13.48
## McMULLEN  10.83      7.31    13.76
##           High.jump X400m
## SEBRLE         2.07 49.81
## CLAY           1.86 49.37
## BERNARD        1.92 48.93
## YURKOV         2.10 50.42
## ZSIVOCZKY      2.01 48.62
## McMULLEN       2.13 49.91
##           X110m.hurdle Discus
## SEBRLE           14.69  43.75
## CLAY             14.05  50.72
## BERNARD          14.99  40.87
## YURKOV           15.31  46.26
## ZSIVOCZKY        14.17  45.67
## McMULLEN         14.38  44.41
##           Pole.vault Javeline
## SEBRLE          5.02    63.19
## CLAY            4.92    60.15
## BERNARD         5.32    62.77
## YURKOV          4.72    63.44
## ZSIVOCZKY       4.42    55.37
## McMULLEN        4.42    56.37
##           X1500m Rank Points
## SEBRLE     291.7    1   8217
## CLAY       301.5    2   8122
## BERNARD    280.1    4   8067
## YURKOV     276.4    5   8036
## ZSIVOCZKY  268.0    7   8004
## McMULLEN   285.1    8   7995
##           Competition
## SEBRLE       Decastar
## CLAY         Decastar
## BERNARD      Decastar
## YURKOV       Decastar
## ZSIVOCZKY    Decastar
## McMULLEN     Decastar
</code></pre></div></div>

<p>Note that, only some of these individuals and variables will be used to
perform the principal component analysis. The coordinates of the
remaining individuals and variables on the factor map will be predicted
after the PCA.</p>

<p>The data used here describes athletes’ performance during two sporting
events (Desctar and OlympicG). It contains 27 individuals (athletes)
described by 13 variables.</p>

<p>In PCA terminology, our data contains:</p>

<ul>
  <li>Active individuals (in light blue, rows 1:23) : Individuals that are
used during the principal component analysis.</li>
  <li>Supplementary individuals (in dark blue, rows 24:27) : The
coordinates of these individuals will be predicted using the PCA
information and parameters obtained with active
individuals/variables</li>
  <li>Active variables (in pink, columns 1:10) : Variables that are used
for the principal component analysis.</li>
  <li>Supplementary variables: As supplementary individuals, the
coordinates of these variables will be predicted also. These can be:
    <ul>
      <li>Supplementary continuous variables (red): Columns 11 and 12
corresponding respectively to the rank and the points of
athletes.</li>
      <li>Supplementary qualitative variables (green): Column 13
corresponding to the two athlete-tic meetings (2004 Olympic Game
or 2004 Decastar). This is a categorical (or factor) variable
factor. It can be used to color individuals by groups.</li>
    </ul>
  </li>
</ul>

<p>We start by subsetting active individuals and active variables for the
principal component analysis:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>decathlon2.active &lt;- decathlon2[1:23, 1:10]
head(decathlon2.active[, 1:6], 4)

##         X100m Long.jump Shot.put
## SEBRLE  11.04      7.58    14.83
## CLAY    10.76      7.40    14.26
## BERNARD 11.02      7.23    14.25
## YURKOV  11.34      7.09    15.19
##         High.jump X400m
## SEBRLE       2.07 49.81
## CLAY         1.86 49.37
## BERNARD      1.92 48.93
## YURKOV       2.10 50.42
##         X110m.hurdle
## SEBRLE         14.69
## CLAY           14.05
## BERNARD        14.99
## YURKOV         15.31
</code></pre></div></div>

<h2 id="data-standardization">Data standardization</h2>

<p>In principal component analysis, variables are often scaled
(i.e. standardized). This is particularly recommended when variables are
measured in different scales (e.g: kilograms, kilometers, centimeters,
…); otherwise, the PCA outputs obtained will be severely affected.</p>

<p>The goal is to make the variables comparable. Generally variables are
scaled to have i) standard deviation one and ii) mean zero.</p>

<p>The standardization of data is an approach widely used in the context of
gene expression data analysis before PCA and clustering analysis. We
might also want to scale the data when the mean and/or the standard
deviation of variables are largely different.</p>

<p>When scaling variables, the data can be transformed as follow:</p>

<p>$\frac{x_i - mean(x)}{sd(x)}$</p>

<p>Where <code class="language-plaintext highlighter-rouge">mean(x)</code> is the mean of <code class="language-plaintext highlighter-rouge">x</code> values, and <code class="language-plaintext highlighter-rouge">sd(x)</code> is the standard
deviation (SD).</p>

<p>The R base function <code class="language-plaintext highlighter-rouge">scale()</code> can be used to standardize the data. It
takes a numeric matrix as an input and performs the scaling on the
columns.</p>

<blockquote>
  <p>Note that, by default, the function <code class="language-plaintext highlighter-rouge">PCA()</code> [in <code class="language-plaintext highlighter-rouge">FactoMineR</code>],
standardizes the data automatically during the PCA; so you don’t need
do this transformation before the PCA.</p>
</blockquote>

<h2 id="analysis">Analysis</h2>

<p>The function <code class="language-plaintext highlighter-rouge">PCA()</code> [<code class="language-plaintext highlighter-rouge">FactoMineR</code> package] can be used. A simplified
format is:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PCA(X, scale.unit = TRUE, ncp = 5, graph = TRUE)
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">X</code>: a data frame. Rows are individuals and columns are numeric
variables</li>
  <li><code class="language-plaintext highlighter-rouge">scale.unit</code>: a logical value. If TRUE, the data are scaled to unit
variance before the analysis. This standardization to the same scale
avoids some variables to become dominant just because of their large
measurement units. It makes variable comparable.</li>
  <li><code class="language-plaintext highlighter-rouge">ncp</code>: number of dimensions kept in the final results.</li>
  <li><code class="language-plaintext highlighter-rouge">graph</code>: a logical value. If TRUE a graph is displayed.</li>
</ul>

<p>The R code below, computes principal component analysis on the active
individuals/variables:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>library("FactoMineR")
decathlon.active &lt;- decathlon2[1:23, 1:10]
res.pca &lt;- PCA(decathlon2.active, graph = TRUE, scale.unit = TRUE)
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-5-1.svg" alt="" /><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-5-2.svg" alt="" /></p>

<p>The output of the function PCA() is a list, including the following
components:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(res.pca)

## **Results for the Principal Component Analysis (PCA)**
## The analysis was performed on 23 individuals, described by 10 variables
## *The results are available in the following objects:
## 
##    name              
## 1  "$eig"            
## 2  "$var"            
## 3  "$var$coord"      
## 4  "$var$cor"        
## 5  "$var$cos2"       
## 6  "$var$contrib"    
## 7  "$ind"            
## 8  "$ind$coord"      
## 9  "$ind$cos2"       
## 10 "$ind$contrib"    
## 11 "$call"           
## 12 "$call$centre"    
## 13 "$call$ecart.type"
## 14 "$call$row.w"     
## 15 "$call$col.w"     
##    description                          
## 1  "eigenvalues"                        
## 2  "results for the variables"          
## 3  "coord. for the variables"           
## 4  "correlations variables - dimensions"
## 5  "cos2 for the variables"             
## 6  "contributions of the variables"     
## 7  "results for the individuals"        
## 8  "coord. for the individuals"         
## 9  "cos2 for the individuals"           
## 10 "contributions of the individuals"   
## 11 "summary statistics"                 
## 12 "mean of the variables"              
## 13 "standard error of the variables"    
## 14 "weights for the individuals"        
## 15 "weights for the variables"
</code></pre></div></div>

<h2 id="visualization-and-interpretation">Visualization and Interpretation</h2>

<p>We’ll use the <code class="language-plaintext highlighter-rouge">factoextra</code> R package to help in the interpretation of
PCA. No matter what function you decide to use <code class="language-plaintext highlighter-rouge">[stats::prcomp()</code>,
<code class="language-plaintext highlighter-rouge">FactoMiner::PCA()</code>, <code class="language-plaintext highlighter-rouge">ade4::dudi.pca()</code>, <code class="language-plaintext highlighter-rouge">ExPosition::epPCA()</code>], you
can easily extract and visualize the results of PCA using R functions
provided in the <code class="language-plaintext highlighter-rouge">factoextra</code> R package.</p>

<p>These functions include:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">get_eigenvalue(res.pca)</code>: Extract the eigenvalues/variances of
principal components</li>
  <li><code class="language-plaintext highlighter-rouge">fviz_eig(res.pca)</code>: Visualize the eigenvalues</li>
  <li><code class="language-plaintext highlighter-rouge">get_pca_ind(res.pca)</code>, <code class="language-plaintext highlighter-rouge">get_pca_var(res.pca)</code>: Extract the results
for individuals and variables, respectively.</li>
  <li><code class="language-plaintext highlighter-rouge">fviz_pca_ind(res.pca)</code>, <code class="language-plaintext highlighter-rouge">fviz_pca_var(res.pca)</code>: Visualize the
results individuals and variables, respectively.</li>
  <li><code class="language-plaintext highlighter-rouge">fviz_pca_biplot(res.pca)</code>: Make a biplot of individuals and
variables.</li>
</ul>

<p>In the next sections, we’ll illustrate each of these functions.</p>

<h2 id="eigenvalues--variances"><code class="language-plaintext highlighter-rouge">Eigenvalues</code> / <code class="language-plaintext highlighter-rouge">Variances</code></h2>

<p>As described in previous sections, the <code class="language-plaintext highlighter-rouge">eigenvalues</code> measure the amount
of variation retained by each principal component. <code class="language-plaintext highlighter-rouge">Eigenvalues</code> are
large for the first PCs and small for the subsequent PCs. That is, the
first PCs corresponds to the directions with the maximum amount of
variation in the data set.</p>

<p>We examine the <code class="language-plaintext highlighter-rouge">eigenvalues</code> to determine the number of principal
components to be considered. The eigenvalues and the proportion of
variances (i.e., information) retained by the principal components (PCs)
can be extracted using the function <code class="language-plaintext highlighter-rouge">get_eigenvalue()</code> [<code class="language-plaintext highlighter-rouge">factoextra</code>
package].</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>library("factoextra")
eig.val &lt;- get_eigenvalue(res.pca)
eig.val

##        eigenvalue variance.percent
## Dim.1   4.1242133        41.242133
## Dim.2   1.8385309        18.385309
## Dim.3   1.2391403        12.391403
## Dim.4   0.8194402         8.194402
## Dim.5   0.7015528         7.015528
## Dim.6   0.4228828         4.228828
## Dim.7   0.3025817         3.025817
## Dim.8   0.2744700         2.744700
## Dim.9   0.1552169         1.552169
## Dim.10  0.1219710         1.219710
##        cumulative.variance.percent
## Dim.1                     41.24213
## Dim.2                     59.62744
## Dim.3                     72.01885
## Dim.4                     80.21325
## Dim.5                     87.22878
## Dim.6                     91.45760
## Dim.7                     94.48342
## Dim.8                     97.22812
## Dim.9                     98.78029
## Dim.10                   100.00000
</code></pre></div></div>

<p>The sum of all the eigenvalues give a total variance of 10.</p>

<p>The proportion of variation explained by each eigenvalue is given in the
second column. For example, 4.124 divided by 10 equals 0.4124, or, about
41.24% of the variation is explained by this first eigenvalue. The
cumulative percentage explained is obtained by adding the successive
proportions of variation explained to obtain the running total. For
instance, 41.242% plus 18.385% equals 59.627%, and so forth. Therefore,
about 59.627% of the variation is explained by the first two eigenvalues
together.</p>

<p>Eigenvalues can be used to determine the number of principal components
to retain after PCA (Kaiser 1961):</p>

<ul>
  <li>
    <p>An eigenvalue &gt; 1 indicates that PCs account for more variance
than accounted by one of the original variables in standardized
data. This is commonly used as a cutoff point for which PCs are
retained. This holds true only when the data are standardized.</p>
  </li>
  <li>
    <p>You can also limit the number of component to that number that
accounts for a certain fraction of the total variance. For example,
if you are satisfied with 70% of the total variance explained then
use the number of components to achieve that.</p>
  </li>
</ul>

<p>Unfortunately, there is no well-accepted objective way to decide how
many principal components are enough. This will depend on the specific
field of application and the specific data set. In practice, we tend to
look at the first few principal components in order to find interesting
patterns in the data.</p>

<p>In our analysis, the first three principal components explain 72% of the
variation. This is an acceptably large percentage.</p>

<p>An alternative method to determine the number of principal components is
to look at a Scree Plot, which is the plot of eigenvalues ordered from
largest to the smallest. The number of component is determined at the
point, beyond which the remaining eigenvalues are all relatively small
and of comparable size.</p>

<p>The scree plot can be produced using the function <code class="language-plaintext highlighter-rouge">fviz_eig()</code> or
<code class="language-plaintext highlighter-rouge">fviz_screeplot()</code> [<code class="language-plaintext highlighter-rouge">factoextra</code> package].</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-8-1.svg" alt="" /></p>

<blockquote>
  <p>From the plot above, we might want to stop at the fifth principal
component. 87% of the information (variances) contained in the data
are retained by the first five principal components.</p>
</blockquote>

<h2 id="graph-of-variables">Graph of variables</h2>

<h3 id="results">Results</h3>

<p>A simple method to extract the results, for variables, from a PCA output
is to use the function<code class="language-plaintext highlighter-rouge">get_pca_var()</code> [<code class="language-plaintext highlighter-rouge">factoextra</code> package]. This
function provides a list of matrices containing all the results for the
active variables (coordinates, correlation between variables and axes,
squared cosine and contributions).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>var &lt;- get_pca_var(res.pca)
var

## Principal Component Analysis Results for variables
##  ===================================================
##   Name      
## 1 "$coord"  
## 2 "$cor"    
## 3 "$cos2"   
## 4 "$contrib"
##   Description                                    
## 1 "Coordinates for the variables"                
## 2 "Correlations between variables and dimensions"
## 3 "Cos2 for the variables"                       
## 4 "contributions of the variables"
</code></pre></div></div>

<p>The components of the <code class="language-plaintext highlighter-rouge">get_pca_var()</code> can be used in the plot of
variables as follow:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">var$coord</code>: coordinates of variables to create a scatter plot</li>
  <li><code class="language-plaintext highlighter-rouge">var$cos2</code>: represents the quality of representation for variables
on the factor map. It’s calculated as the squared coordinates:
var.cos2 = var.coord * var.coord.</li>
  <li>v<code class="language-plaintext highlighter-rouge">ar$contrib</code>: contains the contributions (in percentage) of the
variables to the principal components. The contribution of a
variable (var) to a given principal component is (in percentage) :
(var.cos2 * 100) / (total cos2 of the component).</li>
</ul>

<blockquote>
  <p>Note that, it’s possible to plot variables and to color them according
to either i) their quality on the factor map (cos2) or ii) their
contribution values to the principal components (contrib).</p>
</blockquote>

<p>The different components can be accessed as follow:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Coordinates
head(var$coord)

##                   Dim.1
## X100m        -0.8506257
## Long.jump     0.7941806
## Shot.put      0.7339127
## High.jump     0.6100840
## X400m        -0.7016034
## X110m.hurdle -0.7641252
##                    Dim.2
## X100m        -0.17939806
## Long.jump     0.28085695
## Shot.put      0.08540412
## High.jump    -0.46521415
## X400m         0.29017826
## X110m.hurdle -0.02474081
##                   Dim.3
## X100m         0.3015564
## Long.jump    -0.1905465
## Shot.put      0.5175978
## High.jump     0.3300852
## X400m         0.2835329
## X110m.hurdle  0.4488873
##                    Dim.4
## X100m         0.03357320
## Long.jump    -0.11538956
## Shot.put      0.12846837
## High.jump     0.14455012
## X400m         0.43082552
## X110m.hurdle -0.01689589
##                   Dim.5
## X100m        -0.1944440
## Long.jump     0.2331567
## Shot.put     -0.2488129
## High.jump     0.4027002
## X400m         0.1039085
## X110m.hurdle  0.2242200

# Cos2: quality on the factore map
head(var$cos2)

##                  Dim.1
## X100m        0.7235641
## Long.jump    0.6307229
## Shot.put     0.5386279
## High.jump    0.3722025
## X400m        0.4922473
## X110m.hurdle 0.5838873
##                     Dim.2
## X100m        0.0321836641
## Long.jump    0.0788806285
## Shot.put     0.0072938636
## High.jump    0.2164242070
## X400m        0.0842034209
## X110m.hurdle 0.0006121077
##                   Dim.3
## X100m        0.09093628
## Long.jump    0.03630798
## Shot.put     0.26790749
## High.jump    0.10895622
## X400m        0.08039091
## X110m.hurdle 0.20149984
##                     Dim.4
## X100m        0.0011271597
## Long.jump    0.0133147506
## Shot.put     0.0165041211
## High.jump    0.0208947375
## X400m        0.1856106269
## X110m.hurdle 0.0002854712
##                   Dim.5
## X100m        0.03780845
## Long.jump    0.05436203
## Shot.put     0.06190783
## High.jump    0.16216747
## X400m        0.01079698
## X110m.hurdle 0.05027463

# Contributions to the principal components
head(var$contrib)

##                  Dim.1      Dim.2
## X100m        17.544293  1.7505098
## Long.jump    15.293168  4.2904162
## Shot.put     13.060137  0.3967224
## High.jump     9.024811 11.7715838
## X400m        11.935544  4.5799296
## X110m.hurdle 14.157544  0.0332933
##                  Dim.3       Dim.4
## X100m         7.338659  0.13755240
## Long.jump     2.930094  1.62485936
## Shot.put     21.620432  2.01407269
## High.jump     8.792888  2.54987951
## X400m         6.487636 22.65090599
## X110m.hurdle 16.261261  0.03483735
##                  Dim.5
## X100m         5.389252
## Long.jump     7.748815
## Shot.put      8.824401
## High.jump    23.115504
## X400m         1.539012
## X110m.hurdle  7.166193
</code></pre></div></div>

<p>In this section, we describe how to visualize variables and draw
conclusions about their correlations. Next, we highlight variables
according to either i) their quality of representation on the factor map
or ii) their contributions to the principal components.</p>

<h2 id="correlation-circle">Correlation circle</h2>

<p>The correlation between a variable and a principal component (PC) is
used as the coordinates of the variable on the PC. The representation of
variables differs from the plot of the observations: The observations
are represented by their projections, but the variables are represented
by their correlations (Abdi and Williams 2010).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Coordinates of variables
head(var$coord, 4)

##                Dim.1       Dim.2
## X100m     -0.8506257 -0.17939806
## Long.jump  0.7941806  0.28085695
## Shot.put   0.7339127  0.08540412
## High.jump  0.6100840 -0.46521415
##                Dim.3      Dim.4
## X100m      0.3015564  0.0335732
## Long.jump -0.1905465 -0.1153896
## Shot.put   0.5175978  0.1284684
## High.jump  0.3300852  0.1445501
##                Dim.5
## X100m     -0.1944440
## Long.jump  0.2331567
## Shot.put  -0.2488129
## High.jump  0.4027002
</code></pre></div></div>

<p>To plot variables, type this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fviz_pca_var(res.pca, col.var = "black")
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-12-1.svg" alt="" /></p>

<p>The plot above is also known as variable correlation plots. It shows the
relationships between all variables. It can be interpreted as follow:</p>

<ul>
  <li>Positively correlated variables are grouped together.</li>
  <li>Negatively correlated variables are positioned on opposite sides of
the plot origin (opposed quadrants).</li>
  <li>The distance between variables and the origin measures the quality
of the variables on the factor map. Variables that are away from the
origin are well represented on the factor map.</li>
</ul>

<h2 id="quality-of-representation">Quality of representation</h2>

<p>The quality of representation of the variables on factor map is called
cos2 (square cosine, squared coordinates) . You can access to the cos2
as follow:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>head(var$cos2, 4)

##               Dim.1       Dim.2
## X100m     0.7235641 0.032183664
## Long.jump 0.6307229 0.078880629
## Shot.put  0.5386279 0.007293864
## High.jump 0.3722025 0.216424207
##                Dim.3      Dim.4
## X100m     0.09093628 0.00112716
## Long.jump 0.03630798 0.01331475
## Shot.put  0.26790749 0.01650412
## High.jump 0.10895622 0.02089474
##                Dim.5
## X100m     0.03780845
## Long.jump 0.05436203
## Shot.put  0.06190783
## High.jump 0.16216747
</code></pre></div></div>

<p>You can visualize the cos2 of variables on all the dimensions using the
corrplot package:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>library("corrplot")

## corrplot 0.84 loaded

corrplot(var$cos2, is.corr=FALSE)
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-14-1.svg" alt="" /></p>

<p>It’s also possible to create a bar plot of variables cos2 using the
function <code class="language-plaintext highlighter-rouge">fviz_cos2()</code> [in <code class="language-plaintext highlighter-rouge">factoextra</code>]:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Total cos2 of variables on Dim.1 and Dim.2
fviz_cos2(res.pca, choice = "var", axes = 1:2)
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-15-1.svg" alt="" /></p>

<p>Note that,</p>

<ul>
  <li>
    <p>A high cos2 indicates a good representation of the variable on the
principal component. In this case the variable is positioned close
to the circumference of the correlation circle.</p>
  </li>
  <li>
    <p>A low cos2 indicates that the variable is not perfectly represented
by the PCs. In this case the variable is close to the center of the
circle.</p>
  </li>
</ul>

<p>For a given variable, the sum of the cos2 on all the principal
components is equal to one.</p>

<p>If a variable is perfectly represented by only two principal components
(Dim.1 &amp; Dim.2), the sum of the cos2 on these two PCs is equal to one.
In this case the variables will be positioned on the circle of
correlations.</p>

<p>For some of the variables, more than 2 components might be required to
perfectly represent the data. In this case the variables are positioned
inside the circle of correlations.</p>

<p>In summary:</p>

<ul>
  <li>The cos2 values are used to estimate the quality of the
representation</li>
  <li>The closer a variable is to the circle of correlations, the better
its representation on the factor map (and the more important it is
to interpret these components)</li>
  <li>Variables that are closed to the center of the plot are less
important for the first components.</li>
</ul>

<p>It’s possible to color variables by their cos2 values using the argument
col.var = “cos2”. This produces a gradient colors. In this case, the
argument gradient.cols can be used to provide a custom color. For
instance, gradient.cols = c(“white”, “blue”, “red”) means that:</p>

<ul>
  <li>variables with low cos2 values will be colored in “white”</li>
  <li>variables with mid cos2 values will be colored in “blue”</li>
  <li>variables with high cos2 values will be colored in red</li>
</ul>

<!-- -->

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Color by cos2 values: quality on the factor map
fviz_pca_var(res.pca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE # Avoid text overlapping
             )
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-16-1.svg" alt="" /></p>

<p>Note that, it’s also possible to change the transparency of the
variables according to their cos2 values using the option alpha.var =
“cos2”. For example, type this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Change the transparency by cos2 values
fviz_pca_var(res.pca, alpha.var = "cos2")
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-17-1.svg" alt="" /></p>

<h2 id="contributions-of-variables-to-pcs">Contributions of variables to PCs</h2>

<p>The contributions of variables in accounting for the variability in a
given principal component are expressed in percentage.</p>

<ul>
  <li>Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e.,
Dim.2) are the most important in explaining the variability in the
data set.</li>
  <li>Variables that do not correlated with any PC or correlated with the
last dimensions are variables with low contribution and might be
removed to simplify the overall analysis.</li>
</ul>

<p>The contribution of variables can be extracted as follow:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>head(var$contrib, 4)

##               Dim.1      Dim.2
## X100m     17.544293  1.7505098
## Long.jump 15.293168  4.2904162
## Shot.put  13.060137  0.3967224
## High.jump  9.024811 11.7715838
##               Dim.3     Dim.4
## X100m      7.338659 0.1375524
## Long.jump  2.930094 1.6248594
## Shot.put  21.620432 2.0140727
## High.jump  8.792888 2.5498795
##               Dim.5
## X100m      5.389252
## Long.jump  7.748815
## Shot.put   8.824401
## High.jump 23.115504
</code></pre></div></div>

<blockquote>
  <p>The larger the value of the contribution, the more the variable
contributes to the component.</p>
</blockquote>

<p>It’s possible to use the function corrplot() [corrplot package] to
highlight the most contributing variables for each dimension:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>library("corrplot")
corrplot(var$contrib, is.corr=FALSE)   
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-19-1.svg" alt="" /></p>

<p>The function <code class="language-plaintext highlighter-rouge">fviz_contrib()</code> [<code class="language-plaintext highlighter-rouge">factoextra</code> package] can be used to
draw a bar plot of variable contributions. If your data contains many
variables, you can decide to show only the top contributing variables.
The R code below shows the top 10 variables contributing to the
principal components:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Contributions of variables to PC1
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-20-1.svg" alt="" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Contributions of variables to PC2
fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-20-2.svg" alt="" /></p>

<p>The total contribution to PC1 and PC2 is obtained with the following R
code:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fviz_contrib(res.pca, choice = "var", axes = 1:2, top = 10)
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-21-1.svg" alt="" /></p>

<p>The red dashed line on the graph above indicates the expected average
contribution. If the contribution of the variables were uniform, the
expected value would be 1/length(variables) = 1/10 = 10%. For a given
component, a variable with a contribution larger than this cutoff could
be considered as important in contributing to the component.</p>

<p>Note that, the total contribution of a given variable, on explaining the
variations retained by two principal components, say PC1 and PC2, is
calculated as <code class="language-plaintext highlighter-rouge">contrib = [(C1 * Eig1) + (C2 * Eig2)]/(Eig1 + Eig2)</code>,
where</p>

<ul>
  <li>C1 and C2 are the contributions of the variable on PC1 and PC2,
respectively</li>
  <li>Eig1 and Eig2 are the eigenvalues of PC1 and PC2, respectively.
Recall that eigenvalues measure the amount of variation retained by
each PC.</li>
</ul>

<p>In this case, the expected average contribution (cutoff) is calculated
as follow: As mentioned above, if the contributions of the 10 variables
were uniform, the expected average contribution on a given PC would be
1/10 = 10%. The expected average contribution of a variable for PC1 and
PC2 is : <code class="language-plaintext highlighter-rouge">[(10* Eig1) + (10 * Eig2)]/(Eig1 + Eig2)</code>.</p>

<blockquote>
  <p>It can be seen that the variables - X100m, Long.jump and Pole.vault -
contribute the most to the dimensions 1 and 2.</p>
</blockquote>

<p>The most important (or, contributing) variables can be highlighted on
the correlation plot as follow:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-22-1.svg" alt="" /></p>

<p>Note that, it’s also possible to change the transparency of variables
according to their contrib values using the option alpha.var =
“contrib”. For example, type this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Change the transparency by contrib values
fviz_pca_var(res.pca, alpha.var = "contrib")
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-23-1.svg" alt="" /></p>

<h2 id="color-by-a-custom-continuous-variable">Color by a custom continuous variable</h2>

<p>In the previous sections, we showed how to color variables by their
contributions and their cos2. Note that, it’s possible to color
variables by any custom continuous variable. The coloring variable
should have the same length as the number of active variables in the PCA
(here n = 10).</p>

<p>For example, type this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Create a random continuous variable of length 10
set.seed(123)
my.cont.var &lt;- rnorm(10)
# Color variables by the continuous variable
fviz_pca_var(res.pca, col.var = my.cont.var,
             gradient.cols = c("blue", "yellow", "red"),
             legend.title = "Cont.Var")
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-24-1.svg" alt="" /></p>

<h2 id="color-by-groups">Color by groups</h2>

<p>It’s also possible to change the color of variables by groups defined by
a qualitative/categorical variable, also called factor in R terminology.</p>

<p>As we don’t have any grouping variable in our data sets for classifying
variables, we’ll create it.</p>

<p>In the following demo example, we start by classifying the variables
into 3 groups using the kmeans clustering algorithm. Next, we use the
clusters returned by the kmeans algorithm to color variables.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Create a grouping variable using kmeans
# Create 3 groups of variables (centers = 3)
set.seed(123)
res.km &lt;- kmeans(var$coord, centers = 3, nstart = 25)
grp &lt;- as.factor(res.km$cluster)
# Color variables by groups
fviz_pca_var(res.pca, col.var = grp, 
             palette = c("#0073C2FF", "#EFC000FF", "#868686FF"),
             legend.title = "Cluster")
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-25-1.svg" alt="" /></p>

<h2 id="dimension-description">Dimension description</h2>

<p>In the section PCA variable contributions, we described how to highlight
variables according to their contributions to the principal components.</p>

<p>Note also that, the function <code class="language-plaintext highlighter-rouge">dimdesc()</code> [in <code class="language-plaintext highlighter-rouge">FactoMineR</code>], for
dimension description, can be used to identify the most significantly
associated variables with a given principal component. It can be used as
follow:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>res.desc &lt;- dimdesc(res.pca, axes = c(1,2), proba = 0.05)
# Description of dimension 1
res.desc$Dim.1

## $quanti
##              correlation
## Long.jump      0.7941806
## Discus         0.7432090
## Shot.put       0.7339127
## High.jump      0.6100840
## Javeline       0.4282266
## X400m         -0.7016034
## X110m.hurdle  -0.7641252
## X100m         -0.8506257
##                   p.value
## Long.jump    6.059893e-06
## Discus       4.842563e-05
## Shot.put     6.723102e-05
## High.jump    1.993677e-03
## Javeline     4.149192e-02
## X400m        1.910387e-04
## X110m.hurdle 2.195812e-05
## X100m        2.727129e-07
## 
## attr(,"class")
## [1] "condes" "list "

res.desc$Dim.2

## $quanti
##            correlation
## Pole.vault   0.8074511
## X1500m       0.7844802
## High.jump   -0.4652142
##                 p.value
## Pole.vault 3.205016e-06
## X1500m     9.384747e-06
## High.jump  2.529390e-02
## 
## attr(,"class")
## [1] "condes" "list "
</code></pre></div></div>

<p>In the output above, <code class="language-plaintext highlighter-rouge">$quanti</code> means results for quantitative variables.
Note that, variables are sorted by the <code class="language-plaintext highlighter-rouge">p-value</code> of the correlation.</p>

<h2 id="graph-of-individuals">Graph of individuals</h2>

<p>The results, for individuals can be extracted using the function
<code class="language-plaintext highlighter-rouge">get_pca_ind()</code> [<code class="language-plaintext highlighter-rouge">factoextra</code> package]. Similarly to the
<code class="language-plaintext highlighter-rouge">get_pca_var()</code>, the function <code class="language-plaintext highlighter-rouge">get_pca_ind()</code> provides a list of
matrices containing all the results for the individuals (coordinates,
correlation between individuals and axes, squared cosine and
contributions).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ind &lt;- get_pca_ind(res.pca)
ind

## Principal Component Analysis Results for individuals
##  ===================================================
##   Name      
## 1 "$coord"  
## 2 "$cos2"   
## 3 "$contrib"
##   Description                       
## 1 "Coordinates for the individuals" 
## 2 "Cos2 for the individuals"        
## 3 "contributions of the individuals"

# Coordinates of individuals
head(ind$coord)

##                Dim.1      Dim.2
## SEBRLE     0.1955047  1.5890567
## CLAY       0.8078795  2.4748137
## BERNARD   -1.3591340  1.6480950
## YURKOV    -0.8889532 -0.4426067
## ZSIVOCZKY -0.1081216 -2.0688377
## McMULLEN   0.1212195 -1.0139102
##                Dim.3       Dim.4
## SEBRLE     0.6424912  0.08389652
## CLAY      -1.3873827  1.29838232
## BERNARD    0.2005584 -1.96409420
## YURKOV     2.5295843  0.71290837
## ZSIVOCZKY -1.3342591 -0.10152796
## McMULLEN  -0.8625170  1.34164291
##                 Dim.5
## SEBRLE     1.16829387
## CLAY      -0.82498206
## BERNARD    0.08419345
## YURKOV     0.40782264
## ZSIVOCZKY -0.20145217
## McMULLEN   1.62151286

# Quality of individuals
head(ind$cos2)

##                 Dim.1      Dim.2
## SEBRLE    0.007530179 0.49747323
## CLAY      0.048701249 0.45701660
## BERNARD   0.197199804 0.28996555
## YURKOV    0.096109800 0.02382571
## ZSIVOCZKY 0.001574385 0.57641944
## McMULLEN  0.002175437 0.15219499
##                 Dim.3       Dim.4
## SEBRLE    0.081325232 0.001386688
## CLAY      0.143628117 0.125791741
## BERNARD   0.004294015 0.411819183
## YURKOV    0.778230322 0.061812637
## ZSIVOCZKY 0.239754152 0.001388216
## McMULLEN  0.110137872 0.266486530
##                  Dim.5
## SEBRLE    0.2689026575
## CLAY      0.0507850580
## BERNARD   0.0007567259
## YURKOV    0.0202279796
## ZSIVOCZKY 0.0054654972
## McMULLEN  0.3892621478

# Contributions of individuals
head(ind$contrib)

##                Dim.1      Dim.2
## SEBRLE    0.04029447  5.9714533
## CLAY      0.68805664 14.4839248
## BERNARD   1.94740183  6.4234107
## YURKOV    0.83308415  0.4632733
## ZSIVOCZKY 0.01232413 10.1217143
## McMULLEN  0.01549089  2.4310854
##                Dim.3       Dim.4
## SEBRLE     1.4483919  0.03734589
## CLAY       6.7537381  8.94458283
## BERNARD    0.1411345 20.46819433
## YURKOV    22.4517396  2.69663605
## ZSIVOCZKY  6.2464325  0.05469230
## McMULLEN   2.6102794  9.55055888
##                 Dim.5
## SEBRLE     8.45894063
## CLAY       4.21794385
## BERNARD    0.04393073
## YURKOV     1.03075263
## ZSIVOCZKY  0.25151025
## McMULLEN  16.29493304
</code></pre></div></div>

<h2 id="plots-quality-and-contribution">Plots: quality and contribution</h2>

<p>The <code class="language-plaintext highlighter-rouge">fviz_pca_ind()</code> is used to produce the graph of individuals. To
create a simple plot, type this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ind &lt;- fviz_pca_ind(res.pca, geom = "point", col.ind = rownames(decathlon2.active))

fviz_pca_ind(res.pca, col.ind = "cos2", 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE) # Avoid text overlapping (slow if many points)
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-29-1.svg" alt="" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fviz_pca_ind(res.pca, pointsize = "cos2", 
             pointshape = 21, fill = "#E7B800",
             repel = TRUE) # Avoid text overlapping (slow if many points)
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-29-2.svg" alt="" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fviz_pca_ind(res.pca, col.ind = "cos2", pointsize = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE) # Avoid text overlapping (slow if many points)
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-29-3.svg" alt="" /></p>

<p><code class="language-plaintext highlighter-rouge">ggplot2</code> plot</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>plot &lt;- ggplot(ind$data, aes(x=x, y=y, color=decathlon2$Competition[1:23], shape=decathlon2$Competition[1:23])) + 
  geom_point(size=3) +
  xlab(paste0("Eixo 1: ", round(res.pca$eig[1,2:2], 1), "% ")) + 
  ylab(paste0("Eixo 2: ", round(res.pca$eig[2,2:2], 1), "% ")) + 
  ##ggtitle("PCA All data") +
  labs(color="Competition", shape="Competition") +  geom_vline(xintercept=0, linetype="dashed") +
  geom_hline( yintercept=0, linetype="dashed")

plot
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-30-1.svg" alt="" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>plot +
  theme_minimal() +
  theme(panel.grid = element_blank(), panel.border = element_rect(fill= "transparent"))
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-30-2.svg" alt="" /></p>

<p>To create a bar plot of the quality of representation (cos2) of
individuals on the factor map, you can use the function fviz_cos2() as
previously described for variables:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fviz_cos2(res.pca, choice = "ind")
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-31-1.svg" alt="" /></p>

<p>To visualize the contribution of individuals to the first two principal
components, type this:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Total contribution on PC1 and PC2
fviz_contrib(res.pca, choice = "ind", axes = 1:2)
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-32-1.svg" alt="" /></p>

<h2 id="color-by-groups-1">Color by groups</h2>

<p>Here, we describe how to color individuals by group. Additionally, we
show how to add concentration <code class="language-plaintext highlighter-rouge">ellipses</code> and <code class="language-plaintext highlighter-rouge">confidence ellipses</code> by
groups. For this, we’ll use the iris data as demo data sets.</p>

<p>The column “Species” will be used as grouping variable. We start by
computing principal component analysis as follow:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># The variable Species (index = 5) is removed
# before PCA analysis
iris.pca &lt;- PCA(iris[,-5], graph = FALSE)
</code></pre></div></div>

<p>In the R code below: the argument habillage or col.ind can be used to
specify the factor variable for coloring the individuals by groups.</p>

<p>To add a concentration ellipse around each group, specify the argument
<code class="language-plaintext highlighter-rouge">addEllipses = TRUE.</code> The argument palette can be used to change group
colors.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fviz_pca_ind(iris.pca,
             geom.ind = "point", # show points only (nbut not "text")
             col.ind = iris$Species, # color by groups
             palette = c("#00AFBB", "#E7B800", "#FC4E07"),
             addEllipses = TRUE, # Concentration ellipses
             legend.title = "Groups"
             )
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-34-1.svg" alt="" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Add confidence ellipses
fviz_pca_ind(iris.pca, geom.ind = "point", col.ind = iris$Species, 
             palette = c("#00AFBB", "#E7B800", "#FC4E07"),
             addEllipses = TRUE, ellipse.type = "confidence",
             legend.title = "Groups"
             )
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-34-2.svg" alt="" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fviz_pca_ind(iris.pca,
             label = "none", # hide individual labels
             habillage = iris$Species, # color by groups
             addEllipses = TRUE, # Concentration ellipses
             palette = "jco"
             )
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-34-3.svg" alt="" /></p>

<h2 id="graphical-parameters">Graphical parameters</h2>

<p>To change easily the graphical of any ggplots, you can use the function
ggpar() [ggpubr package]</p>

<p>The graphical parameters that can be changed using ggpar() include:</p>

<ul>
  <li>Main titles, axis labels and legend titles</li>
  <li>Legend position. Possible values: “top”, “bottom”, “left”, “right”,
“none”.</li>
  <li>Color palette.</li>
  <li>Themes. Allowed values include: <code class="language-plaintext highlighter-rouge">theme_gray(</code>), <code class="language-plaintext highlighter-rouge">theme_bw()</code>,
<code class="language-plaintext highlighter-rouge">theme_minimal()</code>, <code class="language-plaintext highlighter-rouge">theme_classic()</code>, <code class="language-plaintext highlighter-rouge">theme_void()</code>.</li>
</ul>

<!-- -->

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ind.p &lt;- fviz_pca_ind(iris.pca, geom = "point", col.ind = iris$Species)
ggpubr::ggpar(ind.p,
              title = "Principal Component Analysis",
              subtitle = "Iris data set",
              caption = "Source: factoextra",
              xlab = "PC1", ylab = "PC2",
              legend.title = "Species", legend.position = "top",
              ggtheme = theme_gray(), palette = "jco")
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-35-1.svg" alt="" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Biplot
fviz_pca_biplot(res.pca, repel = TRUE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969") # Individuals color
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-35-2.svg" alt="" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fviz_pca_biplot(iris.pca, 
                col.ind = iris$Species, palette = "jco", 
                addEllipses = TRUE, label = "var",
                col.var = "black", repel = TRUE,
                legend.title = "Species")
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-35-3.svg" alt="" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fviz_pca_biplot(iris.pca, 
                # Fill individuals by groups
                geom.ind = "point",
                pointshape = 21,
                pointsize = 2.5,
                fill.ind = iris$Species,
                col.ind = "black",
                # Color variable by groups
                col.var = factor(c("sepal", "sepal", "petal", "petal")),
                legend.title = list(fill = "Species", color = "Clusters"),
                repel = TRUE) +       # Avoid label overplotting
  ggpubr::fill_palette("jco") +      # Indiviual fill color
  ggpubr::color_palette("npg")      # Variable colors
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-35-4.svg" alt="" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fviz_pca_biplot(iris.pca, 
                # Individuals
                geom.ind = "point",
                fill.ind = iris$Species, col.ind = "black",
                pointshape = 21, pointsize = 2,
                palette = "jco",
                addEllipses = TRUE,
                # Variables
                alpha.var ="contrib", col.var = "contrib",
                gradient.cols = "RdYlBu", 
                legend.title = list(fill = "Species", color = "Contrib", alpha = "Contrib"))
</code></pre></div></div>

<p><img src="/u9/pca_files/figure-markdown_strict/unnamed-chunk-35-5.svg" alt="" /></p>

<h1 id="reference">Reference:</h1>

<ul>
  <li><a href="https://rpubs.com/sinhrks/plot_pca">Plotting PCA/clustering results using ggplot2 and
ggfortify</a></li>
</ul>

						</div><!-- /.content -->
					</div><!-- /.col -->
					<div class="col-md-4 col-md-offset-1">
						<div class="sections-list-wrapper">
							<div class="sections-list js-sections js-affix js-scrollspy hidden-xs hidden-sm"></div><!-- /.sections-list -->
						</div>
					</div><!-- /.col -->
				</div><!-- /.row -->
			</div><!-- /.container -->
		</div><!-- /.section -->
		
		<div class="js-footer-area">
			
				<nav class="page-nav">
					<div class="container">
						<div class="row">
							<div class="col-xs-12">
								
									<a href="/u8/" class="page-nav__item page-nav__item--prev">
										<i class="icon icon--arrow-left"></i>
										Graphics and Visualization
									</a><!-- /.page-nav__item -->
								
								
									<a href="/projects/" class="page-nav__item page-nav__item--next">
										Projects
										<i class="icon icon--arrow-right"></i>
									</a><!-- /.page-nav__item -->
								
							</div><!-- /.col -->
						</div><!-- /.row -->
					</div><!-- /.container -->
				</nav><!-- /.page-nav -->
			
			
			
	<footer class="site-footer">
		<div class="container">
			<div class="row">
				<div class="col-sm-6">
					
						<a href="/" class="site-footer__logo">R for Plant Physiologists</a>
					
					
						<hr>
						<p class="site-footer__copyright">Contact: <a href="mailto:danicassol@gmail.com">danicassol@gmail.com</a> </br> ©2020 Daniela Cassol. All rights reserved.</p>
					
				</div><!-- /.col -->
				
					<div class="col-sm-6 align-right">
						<ul class="social-list">
							
								<li>
									<a href="https://www.github.com/dcassol" target="_blank" class="social-list__item social-list__item--github">
										<i class="icon icon--github"></i>
									</a>
								</li>
							
						</ul><!-- /.social-list -->
					</div><!-- /.col -->
				
			</div><!-- /.row -->
		</div><!-- /.container -->
	</footer><!-- /.site-footer -->


<script src="/doks-theme/assets/js/vendor/jquery.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/affix.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/bootstrap/scrollspy.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/vendor/matchHeight.min.js"></script>
<script type="text/javascript" src="/doks-theme/assets/js/scripts.min.js"></script>




		</div><!-- /.js-footer-area -->
	</body>
</html>
